{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name - Sarthak Sareen\n",
    "\n",
    "## Student ID - 30761182\n",
    "\n",
    "## ASSIGNEMENT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided three csv file which are follows :\n",
    "1) dirty data file -- 30761182_dirty_data.csv\n",
    "2) missing data file -- 30761182_missing_data\n",
    "3) outier data file -- 30761182_outlier_data\n",
    "\n",
    "We have to perform wrangling techiques in the given file. In the dirty data file we have find out the errors with the help of the exploratory data anaysis and we have to fix the values also.\n",
    "In the missing data file we have to find out the missing values from the dataset and we have to fill the missing values also.\n",
    "In the outlier file we have find out the outlier in the delivery charge column and remove the ouliers rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) LIBRARIES USED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Pandas for making datframes\n",
    "\n",
    "2) nltk for the sentimental analysis\n",
    "\n",
    "3) from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import ast\n",
    "\n",
    "\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) READING THE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file into a dataframe\n",
    "df_new = pd.read_csv(\"30761182_dirty_data.csv\")\n",
    "df_missing = pd.read_csv(\"30761182_missing_data.csv\")\n",
    "df_outlier = pd.read_csv(\"30761182_outlier_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK -DIRTY DATA FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = df_new.columns.tolist()\n",
    "name_miss_cols = df_missing.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#printing the rows and columns of the dataframe\n",
    "print (df_new.shape) \n",
    "#displaying the 5 top rows\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that it has 500 rows and 16 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Analysing the data and fixing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can determine the type of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell this function describes the data with the numerical columns. \n",
    "\n",
    "With the help of this we can find some errors which are as follows:\n",
    "\n",
    "1) customer lat column minimum value is in negative so this states that there are some errors in the customer_lat column as the latitude cannot be <0 as because of the dataset.\n",
    "\n",
    "2) customer long column maximumn value is in positive so this states that there are some errors in the customer_long column as the longitude cannot be > 0 as because of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cel the function helps to describe the dataset and we have included only the categorical data sing include='O'.\n",
    "\n",
    "With the help of this we can see some errors in the dataset :\n",
    "\n",
    "1) Season column have 8 unique values whereas should only have 4 values according to the metadata.\n",
    "\n",
    "2) nearest warehouse have 6 unique values whereas should have only 3 according to the metadata.\n",
    "\n",
    "\n",
    "And with other data we can derive that there are 500 entries in all the columnsand the unique value counts of the specific column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will see for the above columns error that we have found out separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now checking the season column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.season.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the column seson is the there are 8 unique values whereas there should be only 4 according to the metadata.\n",
    "We can see that winter,summer,austum,spring has 7,6,6,1 counts respectively. \n",
    "So with the help of this we can seay that these are errors as the count is less than the other entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Error -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fix the error using the replace function on the dataframe column \n",
    "\n",
    "replacing the following items :\n",
    "winter --  Winter\n",
    "\n",
    "summer -- Summer\n",
    "\n",
    "autumn -- Autumn\n",
    "\n",
    "spring -- Spring\n",
    "\n",
    "With the help of this we wil fix the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.season.replace({'winter':'Winter', 'spring':'Spring', 'summer':'Summer', 'autumn':'Autumn'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see in the above cell there are only 4 unique values according to the metadata.\n",
    "Hence we have fixed the error in the season column of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_new['date']):\n",
    "    date = i.split(\"-\")\n",
    "    if len(date[0]) != 4 or int(date[1]) > 12 or len(date[1]) > 2 or len(date[2]) > 2 or int(date[2]) > 31:\n",
    "        print(date)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that there are errors in the date column which is not appropriate according to the metadata.\n",
    "The date should be in the format of the YYYY-MM-DD. So we have to fix the error and make the date appropriate according to the metadata given.\n",
    "\n",
    "### FIXING THE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date =''\n",
    "def date_swap(split_date):\n",
    "    if int(split_date[2]) > 0 and int(split_date[2]) < 13 and int(split_date[1]) > 12:\n",
    "        split_date[2],split_date[1] = split_date[1],split_date[2]\n",
    "        date = '-'.join(split_date)\n",
    "    if int(split_date[2]) > 0 and int(split_date[2]) < 13 and int(split_date[1]) > 0 and int(split_date[1]) <13:\n",
    "        date = '-'.join(split_date)\n",
    "    else:\n",
    "        date = '-'.join(split_date)\n",
    "    return date\n",
    "    \n",
    "    \n",
    "df_date_new_f = df_new.copy()\n",
    "#looping in the df_new date column using enumerate which gives the index as well    \n",
    "for index,i in enumerate(df_date_new_f['date']):\n",
    "    \n",
    "    #splitting the string into list\n",
    "    split_date = i.split(\"-\")\n",
    "    #checking if the first item length is 4\n",
    "    if len(split_date[0])==4:\n",
    "        #calling function \n",
    "        f_date = date_swap(split_date)\n",
    "        #replacing the date with new correct date format\n",
    "        #print(f_date)\n",
    "        df_new.at[index, 'date'] = f_date\n",
    "        \n",
    "    #checking the condition\n",
    "    if len(split_date[1])==4:\n",
    "        #swapping the second element\n",
    "        split_date[0],split_date[1] = split_date[1],split_date[0]\n",
    "        #calling the function\n",
    "        f_date = date_swap(split_date)\n",
    "        #replacing the date with new correct date format\n",
    "       # print(f_date)\n",
    "\n",
    "        df_new.at[index, 'date'] = f_date \n",
    "    #checking the condition\n",
    "    if len(split_date[2])==4:\n",
    "        #swapping the third element\n",
    "        split_date[0],split_date[2] = split_date[2],split_date[0]\n",
    "        #calling teh function\n",
    "        f_date = date_swap(split_date)\n",
    "        #replacing the date with new correct date format\n",
    "       # print(f_date)\n",
    "        df_new.at[index, 'date'] = f_date    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check with the incorrect columns\n",
    "for index,i in enumerate(df_new['date']):\n",
    "    date = i.split(\"-\")\n",
    "    if len(date[0]) != 4 or int(date[1]) > 12 or len(date[1]) > 2 or len(date[2]) > 2 or int(date[2]) > 31:\n",
    "        print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that the dates are now correscted according to the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the date column to datetime format for further analysis\n",
    "df_new['date'] = pd.to_datetime(df_new['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for season column\n",
    "def season(sea):\n",
    "    if sea in [9,10,11]:\n",
    "        return 'Spring'\n",
    "    if sea in [12,1,2]:\n",
    "        return 'Summer'\n",
    "    if sea in [3,4,5]:\n",
    "        return 'Autumn'\n",
    "    if sea in [6,7,8]:\n",
    "        return 'Winter'\n",
    "\n",
    "df_error_season = df_new.copy()\n",
    "df_error_season['new_season'] = df_error_season['date'].apply(lambda x: season(x.month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_error_season['season'],df_error_season['new_season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that the season ccolumn has errors as we can see that where the season should be Autumn there are 1,2,3 counts of the spring , summer and winter respectively. So we hacve to fix this. And we can see that there are many errors in all other aspects also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXING THE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_error_fix = df_error_season[(df_error_season['new_season'] != df_error_season['season'])]\n",
    "df_new = df_new.drop(df_season_error_fix.index)\n",
    "df_season_error_fix.drop(['season'], axis=1,inplace=True)\n",
    "df_season_error_fix.rename({'new_season':'season'},axis=1, inplace=True)\n",
    "df_new = pd.concat([df_new,df_season_error_fix],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_season_error_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors has been fixed and now all the right season is present in all the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.nearest_warehouse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the column nearest_warehouse is the there are 6 unique values whereas there should be only 3 according to the metadata.\n",
    "We can see that thompson,nickolson,bakers has 5,5,4 counts respectively. \n",
    "So with the help of this we can seay that these are errors as the count is less than the other entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the error - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fix the error using the replace function on the dataframe column \n",
    "\n",
    "replacing the following items :\n",
    "thompson --  Thompson\n",
    "\n",
    "nickolson -- Nickolson\n",
    "\n",
    "bakers -- Bakers\n",
    "\n",
    "\n",
    "With the help of this we wil fix the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXING THE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_warehouse_error = df_new.loc[df_new['nearest_warehouse'].isin(['thompson','nickolson','bakers'])]\n",
    "# df_new = df_new.drop(df_warehouse_error.index)\n",
    "df_new.nearest_warehouse.replace({'thompson':'Thompson', 'nickolson':'Nickolson', 'bakers':'Bakers'},inplace=True)\n",
    "# df_new = pd.concat([df_new,df_warehouse_error],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.nearest_warehouse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see in the above cell there are only 3 unique values according to the metadata.\n",
    "Hence we have fixed the error in the season column of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.is_expedited_delivery.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we checked that does the is expected delivery column have errors or not. So after the value_count function which gives the count of the unique values of the column we can see that it has only two unique columns which is according to the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating object of the function\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column according to the customer review while calculating the polarity compound \n",
    "#score and stating it to True False\n",
    "df_error = df_new.copy()\n",
    "df_error['new_is_happy_customer'] = df_error['latest_customer_review'].apply(lambda x:\n",
    "                                                                         True if sid.polarity_scores(x)['compound']>=0.05 \n",
    "                                                                         else False)\n",
    "#saving the indexes of the error rows in a list for further analysis\n",
    "#error_3 = list(df_new[(df_new['new_is_happy_customer']=='False') & (df_new['is_happy_customer']== True)].index)\n",
    "#creating a cross to check the errors in the new column created and the existing column in df\n",
    "pd.crosstab(df_error['new_is_happy_customer'],df_error['is_happy_customer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above ceel we cann see that there are errors in the is_happy_customer columnn. As where there should be False there were many true values and vice versa.\n",
    "So we will fix the error and change them using the polarity compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making df with not equal in both the columns\n",
    "error_happ_cust = df_error[df_error['new_is_happy_customer'] != df_error['is_happy_customer']]\n",
    "#dropping the rows with indexes which have errors\n",
    "df_new = df_new.drop(error_happ_cust.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the help of the above cross tab we can see that in the column is happy customer has some errors. We can determine the errors as we can see that the new column we created calculating the polarity score compound value using the latest customer review. \n",
    "We can see that there are 28 errors in this column as when there should be False the column has True 10 times and when the column should have True and the coulumn has False this is 18 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the error -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have already created a new column of ith the help of the sentimentintensityanalyzer function if the compound score of the text  >= 0.05 then it's True otherwise 'False'.\n",
    "So we will drop the column which is is_happy_customer and rename the new_is_happy_customer column to :\n",
    "is_happy_customer.\n",
    "With the help of this the errors in the column is_happy_customer gets fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#deleting the column\n",
    "del error_happ_cust['is_happy_customer']\n",
    "#renaming the new column\n",
    "error_happ_cust.rename({'new_is_happy_customer':'is_happy_customer'},axis=1, inplace=True)\n",
    "df_new = pd.concat([df_new,error_happ_cust],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for wrong lat and long values\n",
    "df_new[(df_new['customer_lat'] > 0) & (df_new['customer_long'] < 0)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that there are entries in the longitude and latitude column as lattiude column has values > 0 and the longitude column has <0 entries. That are the errors to be fixed.\n",
    "In the above we can see that there are some rows where the latitude is + and and longitude is negative. We can analyse that the values of the both are exchanged withing themeslves.\n",
    "\n",
    "longitude value of the specific row is in latitude column and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the error -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will swap the values from the longitude column to latitude column where the errors found in the above explorartion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to swap the long and lat values if the condition meets\n",
    "def long_lat_fix(lat,long):\n",
    "    if long < 0 and lat > 0:\n",
    "        long,lat = lat,long \n",
    "    #returning the swapped values\n",
    "    return lat,long\n",
    "df_long_lat_error = df_new[(df_new['customer_lat'] >0) | (df_new['customer_long']<0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new customer latitude column and new customer longitude column and calling the function initialised above using .apply\n",
    "df_long_lat_error['new_customer_lat'],df_long_lat_error['new_customer_long'] = zip(*df_long_lat_error.apply(lambda x: \n",
    "                                                                           long_lat_fix(x['customer_lat'],\n",
    "                                                                                        x['customer_long']), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we have created two new columns of lat and long and fixxing the errors where the values where exchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(df_long_lat_error.index)\n",
    "#droping the customer_lat and cutomer_long column \n",
    "df_long_lat_error.drop(['customer_lat','customer_long'], axis=1,inplace=True)\n",
    "#renaming the columns\n",
    "df_long_lat_error.rename({'new_customer_lat':'customer_lat','new_customer_long':'customer_long'},axis=1, inplace=True)\n",
    "df_new = pd.concat([df_new,df_long_lat_error],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[(df_new['customer_lat'] > 0) & (df_new['customer_long'] < 0)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the above cell we can see that he errors are fixed and there are no errors in the latitude and lonitude colum of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the warehouse datafile\n",
    "df_warehouse = pd.read_csv(\"warehouses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating the distance between the two lat and ong values\n",
    "def dist(lon1,lat1,lon2,lat2):\n",
    "    R = 6378\n",
    "    d_lat = np.radians(lat2-lat1)\n",
    "    d_lon = np.radians(lon2-lon1)\n",
    "    r_lat1 = np.radians(lat1)\n",
    "    r_lat2 = np.radians(lat2)\n",
    "    a = np.sin(d_lat/2.) **2 + np.cos(r_lat1) * np.cos(r_lat2) * np.sin(d_lon/2.)**2\n",
    "    val = 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return round(val,4)\n",
    "#refrence from www.stackoverflow.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to rectify the warehouse name using the distance calculated\n",
    "def cal(long1,lat1):\n",
    "    distance = []\n",
    "    #saving the lat of nickolson warehouse\n",
    "    lat2=df_warehouse[df_warehouse.names=='Nickolson']['lat']\n",
    "    #saving the long of nicholson warehouse\n",
    "    long2=df_warehouse[df_warehouse.names=='Nickolson']['lon']\n",
    "    #calling the function to calc distance\n",
    "    val1 = dist(long1,lat1,long2,lat2)\n",
    "    #appending the distance in a alist\n",
    "    distance.append(val1[0])\n",
    "    \n",
    "    #saving the lat of Thompson warehouse\n",
    "    lat3=df_warehouse[df_warehouse.names=='Thompson']['lat']\n",
    "    #saving the long of Thompson warehouse\n",
    "    long3=df_warehouse[df_warehouse.names=='Thompson']['lon']\n",
    "    #calling the function to calc distance\n",
    "    val2 = dist(long1,lat1,long3,lat3)\n",
    "    #appending the distance in a alist\n",
    "    distance.append(val2[1])\n",
    "    \n",
    "    #saving the lat of Bakers warehouse\n",
    "    lat4=df_warehouse[df_warehouse.names=='Bakers']['lat']\n",
    "    #saving the long of Bakers warehouse\n",
    "    long4= df_warehouse[df_warehouse.names=='Bakers']['lon']\n",
    "    #calling the function to calc distance\n",
    "    val3 = dist(long1,lat1,long4,lat4)\n",
    "    #appending the distance in a alist\n",
    "    distance.append(val3[2])\n",
    "    \n",
    "    #taking the min distance\n",
    "    final_dist = min(distance)\n",
    "    #saving index\n",
    "    warehouse = distance.index(final_dist)\n",
    "    \n",
    "    if warehouse == 0:\n",
    "        warehouse_name = 'Nickolson'\n",
    "    elif warehouse == 1:\n",
    "        warehouse_name = 'Thompson'\n",
    "    elif warehouse == 2:\n",
    "        warehouse_name = 'Bakers'\n",
    "        \n",
    "    return final_dist,warehouse_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_warehouse = df_new.copy()\n",
    "#function call\n",
    "df_error_warehouse['new_distance'],df_error_warehouse['new_warehouse'] = zip(*df_error_warehouse.apply(lambda x: cal(x['customer_long'],\n",
    "                                                                                 x['customer_lat']),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_error_warehouse['new_warehouse'],df_error_warehouse['nearest_warehouse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that in the column nearest warehouse there are errors as there were instances as thw nearest warehouse should be bakers but it was 5 times nickoloson and 2 times thompson. This is applicable for all the warehouses. So we have to fix these errors with the help of the longitude and latitude of and calculating the nearest distance.\n",
    "We have also read the warehouse csv file which gives the long and latitude of the warehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXING THE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_warehouse_fix = df_error_warehouse[df_error_warehouse['nearest_warehouse'] != df_error_warehouse['new_warehouse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(df_error_warehouse_fix.index)\n",
    "df_error_warehouse_fix.drop(['nearest_warehouse','distance_to_nearest_warehouse'], axis=1,inplace=True)\n",
    "df_error_warehouse_fix.rename({'new_warehouse':'nearest_warehouse','new_distance':'distance_to_nearest_warehouse'},\n",
    "                              axis=1, inplace=True)\n",
    "df_new = pd.concat([df_new,df_error_warehouse_fix],sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are removed and the nearest warehouse is now correct in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the dataframe columns shopping cart\n",
    "def split_df(df):\n",
    "    cart=[]\n",
    "    df_cart = df.copy()\n",
    "    for i in df['shopping_cart']:\n",
    "        cart.append(ast.literal_eval(i))\n",
    "    list_cart = []\n",
    "    for i in cart:\n",
    "        #sorting the list \n",
    "        list_cart.append(sorted(i))\n",
    "\n",
    "    final_name =[]\n",
    "    final_qty=[]\n",
    "    length = []\n",
    "    for index,i in enumerate(list_cart):\n",
    "        item_name =[]\n",
    "        qty_list =[]\n",
    "        for j in i:\n",
    "            #appending the quatntity of the cart od specific order into list of list\n",
    "            qty_list.append(j[1])\n",
    "            item_name.append(j[0])\n",
    "            leng = len(item_name)\n",
    "        final_name.append(item_name)\n",
    "        final_qty.append(qty_list)\n",
    "        length.append(leng)\n",
    "    #df_cart = pd.DataFrame()\n",
    "    df_cart['items'] = final_name\n",
    "    df_cart['quantity'] = final_qty\n",
    "    df_cart['length'] = length\n",
    "    df_cart['items'] = df_cart['items'].apply(lambda x: tuple(x))\n",
    "    df_cart['freq'] = df_cart.groupby('items')['items'].transform('size')\n",
    "    #df_cart['price'] = df['order_price']\n",
    "    return df_cart\n",
    "    # df_cart['items'] = df_cart['items'].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make dict for the order items price\n",
    "def dict_for_price(df_cart):\n",
    "    same_no_df = df_cart[df_cart['length']==df_cart['freq']]\n",
    "    same_no_df['new_order_price'] = 0\n",
    "    s_unique_item = same_no_df['items'].value_counts().index.values\n",
    "    price = []\n",
    "    coeff = []\n",
    "    new_d = {}\n",
    "    for i in s_unique_item:\n",
    "        index_list = list(same_no_df[same_no_df['items']==i].index)\n",
    "        s_new_df = same_no_df[same_no_df['items']==i]\n",
    "\n",
    "        try:\n",
    "            #solving linesr regression\n",
    "            coeff = np.array(list(s_new_df.quantity))\n",
    "            price = np.array(list(s_new_df.order_price))\n",
    "            solution = np.linalg.solve(coeff,price)\n",
    "            \n",
    "            #making dictonary with item and price\n",
    "            new_d.update(dict(zip(s_new_df['items'].values[0],solution)))    \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add new col with calculating new order price\n",
    "def to_add_price(dataframe,dicto):\n",
    "    dataframe['new_price_total'] = 0\n",
    "    name_cols_cart = dataframe.columns.tolist()\n",
    "    for j in name_cols_cart:\n",
    "        if j =='items':\n",
    "            index_items = name_cols_cart.index(j)\n",
    "        if j =='quantity':\n",
    "            index_qty = name_cols_cart.index(j)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for i in dataframe.index:\n",
    "        item_cart = []\n",
    "        quant = []\n",
    "        items_cart = dataframe.iloc[i,index_items]\n",
    "        quant = dataframe.iloc[i,index_qty]\n",
    "#         print(items_cart)\n",
    "        for x in items_cart:\n",
    "            #solving the equation using dict with price of the items\n",
    "            dataframe.at[i,'new_price_total'] = dataframe.at[i,'new_price_total'] + dicto[x]*quant[items_cart.index(x)]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function to split data using outlier \n",
    "df_out_split = split_df(df_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dictonary using the split df outlier \n",
    "dictno = dict_for_price(df_out_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see that we have calulated the items price using the outlier data file.\n",
    "\n",
    "We have used the outlier datafile to fincout the price of each items because the order price in the outlier file is correct as comapred to missing and dirty data.\n",
    "\n",
    "With the help of this we find out the price of all the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now calling the function to split the quantities and items in cart using dirty data\n",
    "df_dirty_split = split_df(df_new)\n",
    "\n",
    "df_new = to_add_price(df_dirty_split,dictno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the incorrect order price from the df\n",
    "\n",
    "df_new.drop(['order_price'], axis=1,inplace=True)\n",
    "\n",
    "#using the new calculated order price as the final column\n",
    "df_new.rename({'new_price_total':'order_price'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing irrelevant column \n",
    "df_new.drop(['items','quantity','length','freq'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check whether order total is equal or not\n",
    "def check_order_price(new,old):\n",
    "    if new==old:\n",
    "        return 'True'\n",
    "    else:\n",
    "        return 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_order_total = df_new.copy()\n",
    "\n",
    "#finding the new order total by using the order price and delivery charge and coupon columns\n",
    "df_error_order_total['new_order_total'] =df_error_order_total['order_price'] - (df_error_order_total['order_price']* \n",
    "                                                                                                  df_error_order_total['coupon_discount']/100) + df_error_order_total['delivery_charges']\n",
    "\n",
    "#comparing the original and new calculated order prices\n",
    "df_error_order_total['price_equalitiy'] = df_error_order_total.apply(lambda x: check_order_price(x['new_order_total'],x['order_total']),axis=1)\n",
    "\n",
    "\n",
    "df_error_order_total.price_equalitiy.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we can see that there are 2 rows where the new order total that we have calculated is not equa to the given order total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the old order total column and rename the new order total column this will fix the error in the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirt_order_price = df_error_order_total[df_error_order_total['price_equalitiy']=='False']\n",
    "df_new = df_new.drop(df_dirt_order_price.index)\n",
    "df_dirt_order_price.drop(['order_total','price_equalitiy'], axis=1,inplace=True)\n",
    "df_dirt_order_price.rename({'new_order_total':'order_total'},axis=1, inplace=True)\n",
    "df_new = pd.concat([df_new,df_dirt_order_price],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing the data types and order of the columns as they were before\n",
    "df_new.date = df_new.date.astype(str)\n",
    "df_new = df_new[name_cols]\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing the data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"30761182_dirty_data_solution.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - MISSING DATA FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will analyse and find the missing values in the given dataframe. These missing values can be imputed by using other columns. Example the missing values in season can be calculated by using the months in the date column. We also know that the data given to us is not dirty i.e. it is free of errors. Hence we can use other columns to determine an dimpute the missing values in a partiicular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of the data i.e. rows and columns count\n",
    "df_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about the data like the data types and row counts\n",
    "df_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell we can determine that there are multiple missing values in the above columns where the count is not equal to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing the dataframe\n",
    "df_missing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "df_missing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are\n",
    "\n",
    "55 missing vaues in nearest_warehouse\n",
    "\n",
    "15 missing vaues in order_price\n",
    "\n",
    "40 missing values in delivery_charges\n",
    "\n",
    "15 missing values in order_total\n",
    "\n",
    "31 missing values in distance_to_nearest_warehouse\n",
    "\n",
    "and 40 missing values in is_happy_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the missing values in warehouse column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe that will contain null values in the nearest_warehouse column and distance_to_nearest_warehouse column\n",
    "df_miss_ware = df_missing[(df_missing['nearest_warehouse'].isnull()) | (df_missing['distance_to_nearest_warehouse'].isnull())]\n",
    "\n",
    "#removing the missing values from original dataframe\n",
    "df_missing = df_missing.drop(df_miss_ware.index)\n",
    "\n",
    "#calculating the correct warehouse and distance by using the functions created above in Dirty Data Analysis.\n",
    "df_miss_ware['new_distance'],df_miss_ware['new_warehouse'] = zip(*df_miss_ware.apply(lambda x: cal(x['customer_long'],\n",
    "                                                                                 x['customer_lat']),axis=1))\n",
    "df_miss_ware.drop(['nearest_warehouse','distance_to_nearest_warehouse'], axis=1,inplace=True)\n",
    "df_miss_ware.rename({'new_warehouse':'nearest_warehouse','new_distance':'distance_to_nearest_warehouse'},axis=1, inplace=True)\n",
    "\n",
    "#merging the correct data and original one back together\n",
    "df_missing = pd.concat([df_missing,df_miss_ware],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence all the missing values in the two columns nearest_warehouse and distance_to_nearest_warehouse are imputed with the help of the other column's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "\n",
    "df_missing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the nul values of these columns have been removed now, as shown above\n",
    "\n",
    "Now to find missing values in the column is_happy_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe containing missing values of the column\n",
    "\n",
    "df_miss_cust_happy = df_missing[(df_missing['is_happy_customer'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing value rows from the original dataframe\n",
    "df_missing = df_missing.drop(df_miss_cust_happy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the sentiments from the Sensitivity Intensity Analyser object used above in Dirty Data Analysis\n",
    "\n",
    "df_miss_cust_happy['new_is_happy_customer'] = df_miss_cust_happy['latest_customer_review'].apply(lambda x:\n",
    "                                                                         1.0 if sid.polarity_scores(x)['compound']>=0.05 \n",
    "                                                                         else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the original is_happ_customer column and renaming the calculated one\n",
    "del df_miss_cust_happy['is_happy_customer']\n",
    "df_miss_cust_happy.rename({'new_is_happy_customer':'is_happy_customer'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the correct dataframe now\n",
    "df_missing = pd.concat([df_missing,df_miss_cust_happy],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "\n",
    "df_missing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the missing values in the column order_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the missing price of the order  with the help of the functions we have created in Dirty Data  Analysis which involves splitting the dataframe to get just the quantities and the items. And then using the already created dictionary of items and prices to calculate the final order price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function on missing data\n",
    "df_miss_split = split_df(df_missing)\n",
    "\n",
    "\n",
    "df_missing = to_add_price(df_miss_split,dictno)\n",
    "\n",
    "df_missing.drop(['order_price','items','quantity','length','freq'], axis=1,inplace=True)\n",
    "df_missing.rename({'new_price_total':'order_price'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "\n",
    "df_missing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the missing values in the column order_price have now been removed and we can further use it to analyse the other coumn\n",
    "\n",
    "### Now finding the missing values in the coumn delivery charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deliverchar_miss = df_missing[(df_missing['delivery_charges'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing rows from the dataframe\n",
    "df_missing = df_missing.drop(df_deliverchar_miss.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the new delivery charge with the help of the columns order total, order price and coupon discount\n",
    "\n",
    "df_deliverchar_miss['delivery_charges'] = df_deliverchar_miss['order_total'] - df_deliverchar_miss['order_price'] + (df_deliverchar_miss['order_price']* df_deliverchar_miss['coupon_discount']/100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.concat([df_missing,df_deliverchar_miss],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "\n",
    "df_missing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the missing values in the column delivery charge has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing the order of the columns as they were before\n",
    "df_missing = df_missing[name_miss_cols]\n",
    "df_missing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the file \n",
    "df_missing.to_csv(\"30761182_missing_data_solution.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - OUTLIER DATAFILE\n",
    "\n",
    "In this task we are required to find the outiers in the data given to us only in the column delivery_charge. This column is dependent upon the season, is_expedited_delivery, is_happy_customer and distance to nearest warehouse. We can plot boxplots to determine the outliers based on the given columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the outlier file\n",
    "df_outlier = pd.read_csv(\"30761182_outlier_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dispaying top values\n",
    "df_outlier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for delivery charge w.r.t season\n",
    "df_outlier.boxplot('delivery_charges', by = ['season'] , figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is a variation in the outlier ranges depending upon the different season. The Autumn season and Winter season still seem quite similar. However there is a huge difference for the other two seasons.The outier values are ranging roughly from 160 to 10. \n",
    "\n",
    "Here we cannot simply delete the outliers from the data based on one season as it will make the data in the other season biased. Hence plotting further to see the relationship between the other columns as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery'] , figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the column is_expedited_Delivery has True and False values. The quantile1 and quantile3 ranges are also differering a lot when compared with each other. As a result the iqr looks different for botht the plots. \n",
    "\n",
    "By simpy looking at this we cannot determine the dependency of the delivery charges with this column. Hence we can do further plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier.boxplot('delivery_charges', by = ['is_happy_customer'] , figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the column is_happy_customer has True and False values. The outliers present are very far in range from each other.\n",
    "\n",
    "By simpy looking at this we cannot determine the dependency of the delivery charges with this column. Hence we can do further plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery','is_happy_customer'] , figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier.boxplot('delivery_charges', by = ['season','is_happy_customer'] , figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier.boxplot('delivery_charges', by = ['season','is_happy_customer','is_expedited_delivery'] , figsize=(22, 22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have created a function that will use the dataframe and get the True and Flase combinations of the is_happy_customer and is_expedited_delivery columns. This dataframe is going season by season into the function. Based on this the new dataframes are generated and then another function is called which will calculate the q1,q2 and iqr values. Based on these values the dataframe is again created containing only the values in the delivery_charge column that lie within the given range. This range is\n",
    "\n",
    "q3_val+1.5*iqr\n",
    "\n",
    "and\n",
    "\n",
    "q1_val-1.5*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df):\n",
    "    #True true value of the coumns\n",
    "    df_1 = df[(df['is_happy_customer']==True) & (df['is_expedited_delivery']==True)]\n",
    "    \n",
    "    #function call to remove outliers\n",
    "    final_df_1 = calculate_outlier(df_1)\n",
    "    #True false value of the coumns\n",
    "    df_2 = df[(df['is_happy_customer']==True) & (df['is_expedited_delivery']==False)]\n",
    "    \n",
    "    #function call to remove outliers\n",
    "    final_df_2 = calculate_outlier(df_2)\n",
    "    #False true value of the coumns\n",
    "    df_3 = df[(df['is_happy_customer']==False) & (df['is_expedited_delivery']==True)]\n",
    "    \n",
    "    #function call to remove outliers\n",
    "    final_df_3 = calculate_outlier(df_3)\n",
    "    #False false value of the coumns\n",
    "    df_4 = df[(df['is_happy_customer']==False) & (df['is_expedited_delivery']==False)]\n",
    "    \n",
    "    #function call to remove outliers\n",
    "    final_df_4 = calculate_outlier(df_4)\n",
    "    \n",
    "    #merging the datasets\n",
    "    final_df = pd.concat([final_df_1,final_df_2,final_df_3,final_df_4])\n",
    "    return final_df\n",
    "\n",
    "def calculate_outlier(dataframe):\n",
    "    q1_val = np.quantile(dataframe['delivery_charges'], .25)\n",
    "    q3_val = np.quantile(dataframe['delivery_charges'], .75)\n",
    "    iqr = q3_val-q1_val\n",
    "    \n",
    "    #removing outliers that ie outside the ranges\n",
    "    dataframe = dataframe[(dataframe.delivery_charges <= q3_val+1.5*iqr) & (dataframe.delivery_charges >= q1_val-1.5*iqr)]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframes based on the season\n",
    "\n",
    "df_out_sum = df_outlier[df_outlier['season']=='Summer']\n",
    "df_out_wint = df_outlier[df_outlier['season']=='Winter']\n",
    "df_out_aut = df_outlier[df_outlier['season']=='Autumn']\n",
    "df_out_spr = df_outlier[df_outlier['season']=='Spring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([remove_outlier(df_out_sum),remove_outlier(df_out_wint),\n",
    "                      remove_outlier(df_out_aut),remove_outlier(df_out_spr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data back to file\n",
    "final_df.to_csv(\"30761182_outlier_data_solution.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above three tasks we have learnt how to perform wrangling techiques to the unstructed and dirty data with the help of the metatdata given. We have done three tasks using the given dirty, missing and outlier file. We have used techiques to find out different errors in different columns.\n",
    "\n",
    "The main concepts used in the different tasks are as follows:\n",
    "\n",
    "    TASK 1)\n",
    "\n",
    "      1) Used pandas to wrangle the data and used different function to perform the task and to clean th data. Some of the function used were zip, unique, apply etc.\n",
    "      2) Fixed the errors with the help of the different columns. For example - calculated the nearest distance of the warehiuse and fixing the name of the warehouse.\n",
    "      3) used sentiment analysis function to find out the sentment analysis of the text using the polarity compund score.\n",
    "      4) Writing  the last dataframe to a csv file\n",
    "      \n",
    "    TASK 2) Fill the missing Data \n",
    "        \n",
    "        1) Find out the null values in column is isnull().\n",
    "        2) filled the missing values in different column using the function created in the Task 1.\n",
    "        3) Writing  the last dataframe to a csv file\n",
    "\n",
    "    TASK 3)  Removing the outlier data\n",
    " \n",
    "        1) Analysis of the delivery charge using boxplot.\n",
    "        2) Find out the relationship between the four coumns i.e delivery charge with the season ,is_expeited delivery,is_happy_customer.\n",
    "        3) removing the outlier ith the help of the relationshio bilt with the 4 columns and removing th eoutlier using the iqr formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFRENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) www.stackoverflow.com\n",
    "\n",
    "2) www.geeksforgeeks\n",
    "\n",
    "3) www.towardsdatascience.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
